# from bs4 import BeautifulSoup
# import requests
# from allowCrawl import is_allowed

# def crawl_naver_news(query):
#     base_url = "https://search.naver.com/search.naver"
#     full_url = f"{base_url}?where=news&query={query}"
#     headers = {'User-Agent': 'Mozilla/5.0'}

#     if not is_allowed(full_url):
#         print("robots.txtì— ì˜í•´ í¬ë¡¤ë§ì´ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#         return []

#     res = requests.get(full_url, headers=headers)
#     soup = BeautifulSoup(res.text, 'html.parser')

#     results = []
#     for tag in soup.select("div.sds-comps-vertical-layout sds-comps-full-layout _sghYQmdqcpm83O1jqen"):
#         results.append({
#             'title': tag['title'],
#             'link': tag['href']
#         })

#     print(results)

# if __name__ == "__main__":
#     crawl_naver_news('ì‚¼ì„±ì „ì')
import feedparser

def fetch_google_news(keyword):
    # êµ¬ê¸€ ë‰´ìŠ¤ RSS URL ìƒì„±
    url = f"https://news.google.com/rss/search?q={keyword}&hl=ko&gl=KR&ceid=KR:ko"

    # RSS í”¼ë“œ íŒŒì‹±
    feed = feedparser.parse(url)

    # ê¸°ì‚¬ ê²°ê³¼ ì¶”ì¶œ
    for entry in feed.entries[:5]:  # ìµœê·¼ ë‰´ìŠ¤ 5ê°œë§Œ ì¶œë ¥
        print(f"ğŸ“° {entry.title}")
        print(f"ğŸ•’ {entry.published}")
        print(f"ğŸ”— {entry.link}\n")
        print(f"{entry.summary.split('"')[1]}\n")

# # ì˜ˆì‹œ ì‚¬ìš©
# if __name__ == "__main__":
#     fetch_google_news("ë‰´ìŠ¤ì¼€ì¼")
